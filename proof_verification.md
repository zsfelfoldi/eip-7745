## Validating the search range

The searched block range is part of the query and supplied to the verifier along with the search pattern. The verifier should first determine the proven log value index range and validate it against the specified block range.

The first and last log value or block delimiter entry of the range is always proven. In order to find these, the verifier should find the first and last proven epoch root, and then inside the corresponding `log_entries` trees find the first and last proven log enrty root and reconstruct the bounding log value indices. In the [geth PoC implementation](https://github.com/zsfelfoldi/go-ethereum/blob/proof-poc/core/filtermaps/proof.go) this is implemented in the `findBoundaryIndex` function. If the specified `firstBlock` is zero then the first log value index is expected to be also zero, otherwise it should point to the block delimiter of `firstBlock-1`. If `lastBlock` is `headBlock` then the last log value index is expected to be `nextIndex-1` where `nextIndex` can be found in the right side child of the tree root, while `headBlock` is supplied to the verifier by the caller along with `logIndexRoot`. In all other cases the last index should point to the block delimiter of `lastBlock`.

## Finding potential matches for a single log value

Rows of the filter maps are encoded as lists of column indices in strictly ascending order. These column indices are 24 bits long according to the current [EIP-7745](https://eips.ethereum.org/EIPS/eip-7745) specs, with the higher 16 bits representing the lowest 16 bits of the log value index of the matching log value entry while the lower 8 bits are calculated from the `FNV-1a` hash of the log value index and the log value hash itself, providing additional probabilistic filter strength. Row mapping can change if the number of entries in a row reaches a certain threshold. In this case the entry is placed on a higher mapping layer which means a new log value to row mapping. On higher layers the row length threshold is higher and also the row mapping changes more frequently. Note that row length thresholds for each mapping layer are independent constants from the `ProgressiveList` subtree sizes.

In order to find a certain log value hash, the verifier should look into the rows on mapping layer 0, and if the length of the row reaches the given threshold then is should also look into the higher layer rows. Note that the rows mapped to a given log value at low mapping layers can be longer than the length threshold as they might be also used by another log value in a higher layer mapping. Those entries are not relevant for the given search though. Once the relevant entries are found in the proof, the verifier should check whether the lower 8 bits of the entries match the value based on the `FNV-1a` hash and if they do then add the higher 16 bit map subindices to the set of potential matches. If higher layers need to be accessed then the verifier can also assume that entries of the next layer lower than the last relevant entry of the previous row are irrelevant as they were added before the searched log value was remapped to the next layer.

Note that in order to simplify encoding and proving, entries are encoded as `uint32` little endian even though with the current constants they are only 24 bits long. In the proof format the leaf encoding efficiency can be increased by storing the entries in a more tightly packed form (also including the row length which takes up a full `uint256` chunk) but the hashing format ensures that each leaf contains 8 entries without the binary encoding of individual entries crossing leaf boundaries, thus simplifying the lookup of any specific row entry list index in the tree.

## Pattern matching and final result lookup

Search patterns can specify different log values allowed at different positions (address and/or topics), and also multiple different values allowed at the same position. Once the verifier determines the potential matches for one value, it can do the pattern matching based on these sets of potentially matching log value indices. In the [geth PoC implementation](https://github.com/zsfelfoldi/go-ethereum/blob/proof-poc/core/filtermaps/proof.go) these are implemented in the `potentialMatches` function of the `prover` interface. `singleProver.potentialMatches` returns the potentially matching set for a single log value. `matchAnyProver` gives results for a single position of the search pattern (either the address or one of the topics) by running the single provers of each allowed value at the given position, then returning the union of these sets. `matchSequenceProver` evaluates the `matchAnyProver` of each search pattern position, shifts them left according to the position (for example a match for `topic2` at position 10000 indicates a possible match for the entire log event at position 9998), then returns the intersection of the resulting sets. 

Finally, for each potential match in the final intersection, the proof should contain a merkle path to the individual log entry in the corresponding `log_entries` tree, even if the match is a false positive, which can be determined by looking into the `address` and `topics` fields of the proven log entry. Matching log entry structures are fully proven while in case of a false positive match only a subset of the `address` / `topics` fields might be proven that are sufficient to prove that the log does not match the search pattern. If the log entry at the potentially matching index is entirely missing from the proof or the available fields do not prove that it is not a match and yet the entire log data is not available then the proof is invalid. Also note that a false positive match can point to a position of the `log_entries` tree that does not contain an entry at all (logs are stored at the first log value index of the log). In this case the root of the log entry subtree is a zero hash and the proof should be expected to contain a path to this zero hash. It is also possible that it points to a block delimiter, in which case the generalized tree index 15 of the subtree is proven and contains a `2**64-1` dummy value which makes block delimiters always distinguishable from logs.

## Processing partially proven filter rows

Long rows belonging to higher layer mappings of very popular log values are not fully encoded if the potential match sets of other search pattern positions ensure that any result coming from certain regions of the long row would be irrelevant as the final intersection would not contain any matches from that region anyways. For example if `address` is very popular but `topic2` only has a single potential match at position `15220` then a single leaf from the `address` row containing `[14844, 14967, 14988, 15023, 15503, 15755, 15899, 16010]` proves that there is no match and most of the row data does not need to be proven. It is important to know though that the leaf containing the last entry of previous mapping layers also needs to be proven because if that entry is unknown then any position higher than the last proven entry of the lower layer row is a potential match.

In order to avoid dealing with the complex interactions between the partial proofs of the search pattern positions, the verifier can operate on sets of potential matches that can include continuous ranges as well as individual indices. When processing each individual row, the verifier can iterate on the proven leafs of the row's `ProgressiveList`, and whenever there is an unproven gap in the series of data chunks, it can assume that anything between the last proven entry before the gap and the first proven entry after the gap are potential matches. If union and intersection operations are performed on these sets then these continuous unproven sections should fall out in the end.

Note that the verifier can assume that if any part of a specific row is proven then the `count` field of the `ProgressiveList` is always proven. If no part of the row list is proven then the proof might still be valid. In this case the returned potential matches should cover the entire `0..2**16-1` subrange belonging to the given map index (and it should be expected that the results from the other search pattern positions will prove that no match is possible at all in the given map). The available `ProgressiveList` data chunks should be iterated up to either the actual row length or the row length threshold of the given layer, proven entries should be filtered based on their lower 8 bits, while unproven gaps should be filled with continuous potential match ranges.

If the length of the row is lower than the threshold and the last entry is proven then there is no need to look into higher mapping layers as no further matches are possible. If the length is at least as high as the threshold and the last entry before the threshold is proven then the verifier should progress to the next mapping layer. In either case if the last relevant entry is not proven then it is also not necessary to look into the next layer and any index after the last proven entry, up to the `2**16-1` map subindex range boundary should be considered a potential match.

If the verifier progresses to the next layer, assuming that the last entry of the previous layer row before the threshold was `X`, it should check whether the first entry in the next layer row that is larger than `X` is proven. If this entry is not the first one of the next layer row then in order to verify this condition the last entry smaller than `X` should also be proven. In this case the verifier can proceed processing entries on the next layer without encountering a gap. If this condition is not met though then it is possible that there are further matches between `X` and the first known entry larger than `X`, and therefore it is necessary to consider this range as potential matches.

# Test vectors

In the `proof_test_vectors` directory a number of test vectors are provided. Each test case consists of a `*.json` file containing a description of the test dataset, the log index root hash, the performed filter query and the expected results, along with a `*.proof` binary file containing a Merkle multiproof in the following format:

```
leaf_node_count		uint64
proof_node_count	uint64
leaf_indices		uint128 * leaf_node_count
leaf_nodes		Bytes32 * leaf_node_count
proof_nodes		Bytes32 * proof_node_count	// reverse order
```

Note that all numbers are stored in little endian byte order.

